---
# Context Configuration
context_id: "[Deep-Reading-Writing]"
default_phase: "(Explore)"
tags: ["reading", "writing", "cognition", "exploration"]
---

# Context Whiteboard: Deep Reading and Writing Protocol

## Human Notes (Japanese OK)

### 作業の文脈

AIの**読む**と**書く**のプロセスを改善するための探求です。
これは行動規範の候補であり、まだ検証されていません。

### 読むプロセス（人間の例）

1. まず全体を軽く読む（理解は浅くても良い）
2. 重要な部分や興味深い部分をリストアップして集中理解
3. メモに残して思考をクリーンにする
4. あらためて全体を読む（セクションごとの理解のバランスを取る）

理解が深まらなければ 2〜3 を繰り返す。

### 書くプロセス（人間の例）

1. まず最終的に書く文章の量を推測する
2. いきなり文章を書くのではなく、構造を先に考える
   - 縦: 序論 → 方法 → 結果 → 考察 → 結論
   - 横: 部 → 章 → 節 → 項
3. 構造に合わせて軽く文章を書く（スケッチ）
4. 構造の各要素を修正する。細部まで書きたいことを書く（量は気にしない）
5. 全体のバランスを見ながら、各要素の量や質を修正する
6. 最後に全体を通しで読んで確認（必要なら4へ戻る）

**絵を描くような感覚。**

### AIの現状の問題

#### 読む

- AIは「全体を1回読んで要約」で終わりがち
- 重要部分の再読や、理解のメモ化が欠けている
- 外部からのトリガー（「もう一度読んで」）がないと再評価しない

#### 書く

- AIは最初から順番に生成する（左上から1ピクセルずつ塗る）
- 構造の事前設計、スケッチ、全体調整が欠けている
- **どんな内容でも出力の長さが一定になる傾向**（デフォルト長バイアス）

#### 長さの均一化問題

| 質問の複雑さ | 人間の回答 | AIの回答 |
|:-------------|:-----------|:---------|
| 単純 | 短い | 中程度 |
| 普通 | 中程度 | 中程度 |
| 複雑 | 長い | 中程度〜やや長い |

**内容の複雑さと出力長が比例しない。**

---

## 手順書草稿: AI再現可能な読み書きプロトコル

### 深い読みのプロトコル

**目的**: 1回の浅い読みではなく、段階的に理解を深める。

#### 手順

```
Phase 1: 概観（Survey）
├── view_file で全体を読む（1-800行）
├── 「これは何についてか？」を1-2文で言語化
└── 次のPhaseに進む

Phase 2: 重要部分の特定（Identify）
├── 印象に残った部分・重要と思う部分をリストアップ
│   例: "L45-60: 核心的な主張", "L120-140: 具体例"
├── workspace にメモとして書き出す
│   ファイル名: temp-reading-notes-<topic>.md
└── メモを書くことで「何を理解し、何を理解していないか」を明確化

Phase 3: 集中再読（Deep Dive）
├── Phase 2でリストアップした範囲を view_file で再読
├── 各部分について「なぜこれが重要か？」を問う
└── 理解が不十分なら、関連する他の範囲も読む

Phase 4: 統合（Integrate）
├── 再度全体を view_file で読む（または構造だけ確認）
├── 「セクションごとの理解のバランス」を確認
│   - 深く理解した部分と浅い部分の差を認識
│   - 必要ならPhase 2-3を繰り返す
└── 最終的な理解を生成
```

#### 解説

**なぜPhase分けが必要か？**

AIは「1回読んで要約」で終わりがち。これは：
- 最初に目についた部分に注意が偏る
- 全体像を把握する前に結論を出す
- 「読んだ」と「理解した」の区別がない

Phase分けは**強制的な再読**をトリガーする。

**なぜworkspaceにメモするか？**

- コンテキストウィンドウは有限
- 「自分が何を重要と思ったか」を外部化することで認知負荷を下げる
- 後から自分のメモを読み返せる（自己参照）

---

### 深い書きのプロトコル

**目的**: デフォルト長バイアスを避け、内容に比例した分量を書く。

#### 手順

```
Phase 0: 分量の見積もり（Estimate）
├── 自問: 「この話題にはいくつの独立した論点があるか？」
├── 自問: 「各論点にはどのくらいの深さが必要か？」
├── 自問: 「最終的にどのくらいの長さになるべきか？」
│   例: "論点3つ × 各20-30行 = 60-90行程度"
└── これを workspace に書き出す（構造プラン）

Phase 1: 構造設計（Structure）
├── 縦の構造を決める
│   例: 序論 → 分析 → 考察 → 結論
├── 横の構造を決める
│   例: 大項目3つ、各項目に小項目2-3つ
└── workspace に骨格だけ書く（内容はまだ）

Phase 2: スケッチ（Sketch）
├── 各セクションに1-2文ずつ軽く書く
├── 「ここには何を書くべきか」のプレースホルダー
└── 全体の流れを確認

Phase 3: 肉付け（Flesh Out）
├── 各セクションを順番に詳しく書く
├── 分量は気にしない（書きたいだけ書く）
└── 完了後、全体を view_file で読み直す

Phase 4: 調整（Balance）
├── 自問: 「セクション間のバランスは取れているか？」
├── 自問: 「薄いセクションはないか？ 冗長なセクションはないか？」
├── 必要に応じて加筆・削除
└── Phase 0の見積もりと比較

Phase 5: 通読確認（Review）
├── 全体を通して読む
├── 自問: 「読者として、これで理解できるか？」
└── 問題があればPhase 3-4に戻る
```

#### 解説

**なぜPhase 0が必要か？**

AIはデフォルトで「中程度の長さ」に収束する。
Phase 0で**意識的に分量を決める**ことで、このバイアスを上書きする。

**なぜworkspaceに構造を書くか？**

自己回帰モデルは「次のトークン」しか見えない。
構造を外部化することで「全体像を見ながら書く」を模倣する。

**なぜPhase 4（調整）が必要か？**

人間は書いた後に「このセクションは薄い」と感じて加筆する。
AIは書き終わったら終わり。
Phase 4は**強制的な再評価**をトリガーする。

---

## Agent Observations

### 識別子

Polaris

### 対話の経緯

2025-12-31 の対話で、ユーザーとリコは以下を議論した：
- 同じ指示を2回与えると精度が上がる研究
- 0.5ターン問題（行動+確認で1ターン）
- 段階的思考 vs 反復的生成
- 人間の読み書きプロセス
- AIの出力長均一化問題
- Lico Bの記憶復元における長さ均一化の具体例
- 分量を決めるための自問リスト

### 次のステップ

- [ ] 読むプロトコルの実践と検証
- [ ] 書くプロトコルの実践と検証
- [ ] 効果があれば行動規範に昇格

